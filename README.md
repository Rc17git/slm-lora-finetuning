
# Parameter-Efficient Fine-Tuning of TinyLlama using LoRA (PEFT)

## ğŸ“Œ Project Overview

This project demonstrates **parameter-efficient fine-tuning (PEFT)** of a small open-source language model using **LoRA (Low-Rank Adaptation)**.

Instead of full fine-tuning of all 1.1B parameters, this project:

- Freezes the base model weights  
- Injects trainable LoRA adapters into attention layers  
- Trains only a small fraction of parameters  
- Evaluates improvement over the base model using perplexity  

The objective was to measure how effectively LoRA can specialize a pretrained instruction model using a relatively small dataset.

---

## ğŸ§  Base Model

- **Model:** TinyLlama-1.1B-Chat  
- **Parameters:** ~1.1B  
- **Fine-Tuning Method:** LoRA (PEFT)  

LoRA was applied to: 

target_modules = ["q_proj", "v_proj"]

Only ~0.3â€“0.6% of total parameters were trained.

---

## âš™ï¸ LoRA Configuration

| Parameter | Value |
|------------|--------|
| r | 8 / 16 |
| lora_alpha | 2 Ã— r |
| lora_dropout | 0.05 |
| bias | none |
| task_type | CAUSAL_LM |

---

## ğŸ“Š Dataset

- **Dataset:** Databricks Dolly 15k  
- **Subset Sizes Tested:** 1500 and 3000 samples  
- **Train/Validation Split:** 90/10  

Each sample was formatted as:

Each sample was formatted as:

Instruction:
...
Response:
...

---

## ğŸ‹ï¸ Training Setup

- Epochs: 3  
- Learning Rate: 2e-4  
- Effective Batch Size: 16 (via gradient accumulation)  
- Optimizer: AdamW  
- Mixed Precision: FP16  

---

## ğŸ“ˆ Results

| Model | Dataset Size | r | Epochs | Eval Loss | Perplexity |
|-------|-------------|---|--------|-----------|------------|
| Base TinyLlama | - | - | - | 2.124 | 8.37 |
| LoRA Fine-tuned | 1500 | 8 | 3 | 1.698 | 5.46 |
| LoRA Fine-tuned | 1500 | 16 | 3 | 1.698 | 5.44 |
| LoRA Fine-tuned | 3000 | 16 | 3 | 1.680 | 5.41 |

---

## ğŸ” Analysis

### 1ï¸âƒ£ Perplexity Reduction

The base model achieved:

> **Perplexity: 8.37**

After LoRA fine-tuning:

> **Perplexity: 5.41 â€“ 5.46**

This represents approximately a:

> **35% reduction in perplexity**

This indicates improved alignment with the instruction-response distribution.

---

### 2ï¸âƒ£ Effect of LoRA Rank (r)

Increasing `r` from 8 â†’ 16 resulted in marginal improvement: 5.46 â†’ 5.44

This suggests:

- r=8 already provides sufficient adaptation capacity.
- Additional rank increases lead to diminishing returns.
- The task does not require high-rank adaptation.

---

### 3ï¸âƒ£ Effect of Dataset Size

Increasing dataset size from 1500 â†’ 3000 resulted in: 5.44 â†’ 5.41

This modest improvement suggests:

- TinyLlama-Chat is already instruction-aligned.
- Dolly dataset distribution is stylistically consistent.
- Performance plateaus once format alignment is learned.

---

## ğŸ§  Key Takeaways

- LoRA successfully specialized the model without full fine-tuning.
- Significant perplexity reduction was achieved with minimal trainable parameters.
- Increasing LoRA rank beyond 8 provided limited gains.
- Doubling dataset size produced diminishing returns.
- PEFT is highly effective for lightweight model adaptation.

---

## ğŸš€ Why This Matters

This project demonstrates:

- Practical implementation of PEFT (LoRA)
- Controlled experiment design
- Quantitative evaluation using perplexity
- Hyperparameter comparison
- Analysis of diminishing returns

It highlights the ability to:

- Fine-tune LLMs efficiently
- Design reproducible ML experiments
- Interpret performance metrics critically
- Make informed architectural decisions

---

## ğŸ“‚ Repository Structure

slm-lora-finetuning/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ train.py
â”‚ â”œâ”€â”€ evaluate.py
â”‚ â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ experiment_table.md
â”‚ â””â”€â”€ metrics.json
â”‚
â”œâ”€â”€ adapter/
â”‚ â””â”€â”€ tinyllama-lora-adapter/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.m

---

## ğŸ”® Future Improvements

- Extend LoRA to additional attention modules (`k_proj`, `o_proj`)
- Fine-tune on domain-specific dataset (e.g., ML Q&A)
- Add qualitative output comparison section
- Compare LoRA vs full fine-tuning efficiency
- Deploy inference demo

---