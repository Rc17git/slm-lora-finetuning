## ðŸ“ˆ Results

| Model | Dataset Size | r | Epochs | Eval Loss | Perplexity |
|-------|-------------|---|--------|-----------|------------|
| Base TinyLlama | - | - | - | 2.124 | 8.37 |
| LoRA Fine-tuned | 1500 | 8 | 3 | 1.698 | 5.46 |
| LoRA Fine-tuned | 1500 | 16 | 3 | 1.698 | 5.44 |
| LoRA Fine-tuned | 3000 | 16 | 3 | 1.680 | 5.41 |