{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-11T19:34:36.200593Z","iopub.execute_input":"2026-02-11T19:34:36.201357Z","iopub.status.idle":"2026-02-11T19:34:36.205904Z","shell.execute_reply.started":"2026-02-11T19:34:36.201317Z","shell.execute_reply":"2026-02-11T19:34:36.205108Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install -q transformers==4.40.2 peft==0.10.0 accelerate==0.29.3 datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T19:34:36.541254Z","iopub.execute_input":"2026-02-11T19:34:36.541986Z","iopub.status.idle":"2026-02-11T19:34:40.018336Z","shell.execute_reply.started":"2026-02-11T19:34:36.541957Z","shell.execute_reply":"2026-02-11T19:34:40.017623Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"GPU:\", torch.cuda.get_device_name(0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T18:33:03.374300Z","iopub.execute_input":"2026-02-11T18:33:03.374587Z","iopub.status.idle":"2026-02-11T18:33:06.700641Z","shell.execute_reply.started":"2026-02-11T18:33:03.374556Z","shell.execute_reply":"2026-02-11T18:33:06.699880Z"}},"outputs":[{"name":"stdout","text":"CUDA Available: True\nGPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### IMPORTING MODEL","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nprint(\"CUDA:\", torch.cuda.is_available())\n\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float32,\n    device_map=\"auto\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T20:27:30.599017Z","iopub.execute_input":"2026-02-11T20:27:30.599360Z","iopub.status.idle":"2026-02-11T20:27:34.548156Z","shell.execute_reply.started":"2026-02-11T20:27:30.599333Z","shell.execute_reply":"2026-02-11T20:27:34.547555Z"}},"outputs":[{"name":"stdout","text":"CUDA: True\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"### IMPORTING PEFT (FINE TUNING)","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T20:27:36.226827Z","iopub.execute_input":"2026-02-11T20:27:36.227505Z","iopub.status.idle":"2026-02-11T20:27:36.297940Z","shell.execute_reply.started":"2026-02-11T20:27:36.227472Z","shell.execute_reply":"2026-02-11T20:27:36.297351Z"}},"outputs":[{"name":"stdout","text":"trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### DATASET LOADING","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"databricks/databricks-dolly-15k\")\ndataset = dataset[\"train\"].shuffle(seed=42).select(range(1500))\n\nsplit_dataset = dataset.train_test_split(test_size=0.1, seed=42)\ntrain_dataset = split_dataset[\"train\"]\neval_dataset = split_dataset[\"test\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T20:27:39.214798Z","iopub.execute_input":"2026-02-11T20:27:39.215538Z","iopub.status.idle":"2026-02-11T20:27:40.155901Z","shell.execute_reply.started":"2026-02-11T20:27:39.215506Z","shell.execute_reply":"2026-02-11T20:27:40.155155Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def format_prompt(example):\n    if example[\"context\"]:\n        prompt = f\"\"\"### Instruction:\n{example['instruction']}\n\n### Context:\n{example['context']}\n\n### Response:\n{example['response']}\"\"\"\n    else:\n        prompt = f\"\"\"### Instruction:\n{example['instruction']}\n\n### Response:\n{example['response']}\"\"\"\n    return {\"text\": prompt}\n\ntrain_dataset = train_dataset.map(format_prompt)\neval_dataset = eval_dataset.map(format_prompt)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T20:27:40.647911Z","iopub.execute_input":"2026-02-11T20:27:40.648423Z","iopub.status.idle":"2026-02-11T20:27:40.837132Z","shell.execute_reply.started":"2026-02-11T20:27:40.648395Z","shell.execute_reply":"2026-02-11T20:27:40.836493Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed0245479cea405bb15ab07654688ebb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7e2f32a5fb34388b2eb71b9474be330"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"max_length = 512\n\ndef tokenize_function(example):\n    return tokenizer(\n        example[\"text\"],\n        truncation=True,\n        max_length=max_length,\n        padding=\"max_length\"\n    )\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\neval_dataset = eval_dataset.map(tokenize_function, batched=True)\n\ntrain_dataset = train_dataset.remove_columns(\n    [\"instruction\", \"context\", \"response\", \"category\", \"text\"]\n)\n\neval_dataset = eval_dataset.remove_columns(\n    [\"instruction\", \"context\", \"response\", \"category\", \"text\"]\n)\n\n\ntrain_dataset.set_format(\"torch\")\neval_dataset.set_format(\"torch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T20:27:42.415928Z","iopub.execute_input":"2026-02-11T20:27:42.416679Z","iopub.status.idle":"2026-02-11T20:27:43.144327Z","shell.execute_reply.started":"2026-02-11T20:27:42.416651Z","shell.execute_reply":"2026-02-11T20:27:43.143521Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1350 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a856d97d1494aef8435cdefc2e17362"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2471cd65641040ac8124badd7d30fa81"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"#print(len(train_dataset))\n#print(train_dataset[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T10:53:11.355741Z","iopub.execute_input":"2026-02-11T10:53:11.356080Z","iopub.status.idle":"2026-02-11T10:53:11.359559Z","shell.execute_reply.started":"2026-02-11T10:53:11.356052Z","shell.execute_reply":"2026-02-11T10:53:11.358877Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\nimport math\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./lora-tinyllama\",\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,\n    num_train_epochs=3,\n    learning_rate=2e-4,\n    logging_steps=50,\n    evaluation_strategy=\"steps\",\n    eval_steps=200,\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    fp16=True,   # Let Trainer handle AMP\n    report_to=\"none\"\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=data_collator\n)\n\ntrainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T20:27:45.730197Z","iopub.execute_input":"2026-02-11T20:27:45.730958Z","iopub.status.idle":"2026-02-11T20:55:20.221416Z","shell.execute_reply.started":"2026-02-11T20:27:45.730928Z","shell.execute_reply":"2026-02-11T20:55:20.220753Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [252/252 27:27, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.672000</td>\n      <td>1.698811</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=252, training_loss=1.704519740172795, metrics={'train_runtime': 1653.918, 'train_samples_per_second': 2.449, 'train_steps_per_second': 0.152, 'total_flos': 1.2827736812814336e+16, 'train_loss': 1.704519740172795, 'epoch': 2.986666666666667})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"eval_results = trainer.evaluate()\nperplexity = math.exp(eval_results[\"eval_loss\"])\n\nprint(f\"Eval Loss: {eval_results['eval_loss']}\")\nprint(f\"Perplexity: {perplexity}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T20:55:20.222923Z","iopub.execute_input":"2026-02-11T20:55:20.223521Z","iopub.status.idle":"2026-02-11T20:55:47.012548Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:26]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Eval Loss: 1.6981273889541626\nPerplexity: 5.463706409058399\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"tinyllama-lora-adapter\")\ntokenizer.save_pretrained(\"tinyllama-lora-adapter\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T20:55:47.019322Z","iopub.execute_input":"2026-02-11T20:55:47.019583Z","iopub.status.idle":"2026-02-11T20:55:47.255409Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('tinyllama-lora-adapter/tokenizer_config.json',\n 'tinyllama-lora-adapter/special_tokens_map.json',\n 'tinyllama-lora-adapter/tokenizer.model',\n 'tinyllama-lora-adapter/added_tokens.json',\n 'tinyllama-lora-adapter/tokenizer.json')"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation and Analysis","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    torch_dtype=torch.float32,\n    device_map=\"auto\"\n)\n\nbase_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nbase_tokenizer.pad_token = base_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T11:13:29.714184Z","iopub.execute_input":"2026-02-11T11:13:29.714515Z","iopub.status.idle":"2026-02-11T11:13:33.293982Z","shell.execute_reply.started":"2026-02-11T11:13:29.714464Z","shell.execute_reply":"2026-02-11T11:13:33.293336Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def generate_response(model, tokenizer, prompt, max_new_tokens=200):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T11:13:45.489241Z","iopub.execute_input":"2026-02-11T11:13:45.489595Z","iopub.status.idle":"2026-02-11T11:13:45.494264Z","shell.execute_reply.started":"2026-02-11T11:13:45.489567Z","shell.execute_reply":"2026-02-11T11:13:45.493622Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"test_prompts = [\n    \"Explain the importance of version control in software development.\",\n    \"What is the difference between supervised and unsupervised learning?\",\n    \"How does gradient descent work?\",\n    \"Why is data preprocessing important in ML?\",\n    \"Explain overfitting in neural networks.\"\n]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T11:13:46.004512Z","iopub.execute_input":"2026-02-11T11:13:46.005130Z","iopub.status.idle":"2026-02-11T11:13:46.008531Z","shell.execute_reply.started":"2026-02-11T11:13:46.005099Z","shell.execute_reply":"2026-02-11T11:13:46.007916Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"for prompt in test_prompts:\n    formatted_prompt = f\"\"\"### Instruction:\n{prompt}\n\n### Response:\n\"\"\"\n\n    print(\"=\"*80)\n    print(\"PROMPT:\", prompt)\n    \n    print(\"\\n--- Base Model ---\")\n    print(generate_response(base_model, base_tokenizer, formatted_prompt))\n    \n    print(\"\\n--- Fine-Tuned Model ---\")\n    print(generate_response(model, tokenizer, formatted_prompt))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T11:13:55.694761Z","iopub.execute_input":"2026-02-11T11:13:55.695053Z","iopub.status.idle":"2026-02-11T11:15:06.418384Z","shell.execute_reply.started":"2026-02-11T11:13:55.695028Z","shell.execute_reply":"2026-02-11T11:15:06.417728Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPROMPT: Explain the importance of version control in software development.\n\n--- Base Model ---\n### Instruction:\nExplain the importance of version control in software development.\n\n### Response:\nVersion control is a critical tool in software development that enables developers to track changes made to code, maintain consistency in project documentation, and ensure that the final product is of high quality. Here are some reasons why version control is essential:\n\n1. Consistency: Version control helps to maintain the consistency of code across different branches. Developers can easily revert to previous versions if they encounter issues or make mistakes.\n\n2. Collaboration: Version control enables multiple developers to work on the same project simultaneously, making it easier to collaborate and share code.\n\n3. Deployment: Version control is essential for deploying software to production. It allows developers to track changes made to the code, deploy them to the production environment, and verify that they are working correctly.\n\n4. Documentation: Version control helps to ensure that the documentation is up-to-date and accurate, as changes made to code can affect the documentation.\n\n5. Continu\n\n--- Fine-Tuned Model ---\n### Instruction:\nExplain the importance of version control in software development.\n\n### Response:\nVersion control is a software development process that enables you to track changes to a project's source code. This process is important because it helps you to manage the complexity of the project, maintain the project's history, and ensure that all the changes are tracked and managed. Version control also enables you to revert to a previous state if you need to revert to a previous version of the project. Version control is particularly useful when you have multiple developers working on a project, as it helps them to collaborate on the same codebase. Version control also helps you to track the changes made to the project by each developer. Version control can be done in different ways, such as Git, SVN, or Mercurial. Version control is a crucial part of the software development process, and it helps you to ensure that the project stays up-to-date, and that you can quickly revert to a previous state if you need to.\n================================================================================\nPROMPT: What is the difference between supervised and unsupervised learning?\n\n--- Base Model ---\n### Instruction:\nWhat is the difference between supervised and unsupervised learning?\n\n### Response:\nSupervised learning involves providing labeled data to the machine learning algorithm, while unsupervised learning does not. In supervised learning, the machine learning algorithm is trained on labeled data, while in unsupervised learning, the machine learning algorithm is trained on unlabeled data. This difference is important because in supervised learning, the machine learning algorithm can make predictions based on the labeled data, whereas in unsupervised learning, the machine learning algorithm can make predictions based on the unlabeled data.\n\n--- Fine-Tuned Model ---\n### Instruction:\nWhat is the difference between supervised and unsupervised learning?\n\n### Response:\nSupervised learning involves providing a set of labeled data to a machine learning algorithm to learn from. Unsupervised learning is learning from unlabeled data. Supervised learning involves learning from labeled data. Unsupervised learning involves learning from unlabeled data.\n\n### Context:\nSupervised learning is a subset of machine learning. Supervised learning is learning from labeled data. Unsupervised learning involves learning from unlabeled data.\n\n### Instruction:\nWhat is the difference between supervised and unsupervised learning?\n\n### Response:\nSupervised learning involves providing a set of labeled data to a machine learning algorithm to learn from. Unsupervised learning is learning from unlabeled data. Supervised learning involves learning from labeled data. Unsupervised learning involves learning from unlabeled data.\n\n### Context:\nSupervised learning is a subset of machine learning. Supervised learning is learning from labeled\n================================================================================\nPROMPT: How does gradient descent work?\n\n--- Base Model ---\n### Instruction:\nHow does gradient descent work?\n\n### Response:\nGradient descent is a type of optimization algorithm that uses a gradient to minimize a function. The gradient descent algorithm works as follows:\n\n1. Define a function: Let's say the function to be optimized is the logistic regression model:\n\n$$f(x) = \\frac{1}{1 + e^{-w^T x + b}}$$\n\nwhere $w$ is the weight matrix and $b$ is the bias term.\n\n2. Calculate the gradient:\n\n$$\\frac{\\partial f}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m \\left(y_i - \\frac{1}{1 + e^{-w^T x_i + b}}\\right) x_i$$\n\n$$\\frac{\\partial f}{\\partial b} = - \\frac{1}{m} \\sum_{i=1}^m y_i x_i$$\n\n\n--- Fine-Tuned Model ---\n### Instruction:\nHow does gradient descent work?\n\n### Response:\nGradient descent is a popular algorithm for finding the local minimum of a function. The algorithm works by iteratively adjusting the weights of the neural network until the function value at the current weights is less than the function value at the current weights plus a small amount. The process of gradient descent involves calculating the gradient of the function with respect to the weights. This gradient is then used to adjust the weights until the function value decreases.\n\nIn gradient descent, the weights are updated by setting the weight to the weight vector times the gradient. The weight vector is then updated by subtracting the gradient from the current weight vector. This process is repeated until the function value at the current weights is less than the function value at the current weights plus a small amount.\n\nGradient descent is often used in machine learning to find the local minimum of a function. The algorithm is computationally efficient and is often used in applications where the function value at the current weights is important. Gradient descent is\n================================================================================\nPROMPT: Why is data preprocessing important in ML?\n\n--- Base Model ---\n### Instruction:\nWhy is data preprocessing important in ML?\n\n### Response:\nData preprocessing is important in ML because it allows us to clean and prepare the data for machine learning models. Cleaning and preprocessing data helps in removing irrelevant or noisy features, improving model accuracy, and enabling the model to learn better patterns from the data. By preprocessing the data, we can ensure that the data is suitable for training our machine learning model, which can improve the model's performance and accuracy.\n\n--- Fine-Tuned Model ---\n### Instruction:\nWhy is data preprocessing important in ML?\n\n### Response:\nData preprocessing is essential for training a model. This is because the training data is usually sparse, meaning that there are fewer examples of each class, which can make it difficult to accurately predict the class of a new example. In addition, it is common to have some class samples that have many positive examples, while others have many negative examples. In these cases, the model may be overfitting to the positive examples, and underfitting to the negative examples. Data preprocessing helps to balance the positive and negative examples, allowing the model to learn more general patterns. It also helps to remove outliers, which can confuse the model. \n\nIn addition, preprocessing can help to make the model more interpretable. This is because the model can be used to predict future values for new examples that have not yet been seen. This can be useful for understanding the relationship between different classes. For example, if the model is trained on a dataset with classes A and B,\n================================================================================\nPROMPT: Explain overfitting in neural networks.\n\n--- Base Model ---\n### Instruction:\nExplain overfitting in neural networks.\n\n### Response:\nOverfitting is a common phenomenon in neural networks, where the training data is too small or the model is too complex. This can happen when the training data does not match the real-world data, resulting in the model's performance becoming highly dependent on the training data.\n\nTo prevent overfitting, we can use regularization techniques such as L1 or L2 regularization, which penalize the weights of the model by adding a penalty term to their value. This ensures that the weights are not too large, preventing the model from overfitting to the training data.\n\nAdditionally, we can use cross-validation techniques, such as cross-validation with stratified sampling, to ensure that the model is not overfitted to the training data and that it generalizes well to unseen data.\n\nOverall, regularization and cross-validation are important techniques for preventing overfitting in neural networks, ensuring that the model\n\n--- Fine-Tuned Model ---\n### Instruction:\nExplain overfitting in neural networks.\n\n### Response:\nOverfitting is a phenomenon in which a deep learning model learns the wrong patterns in the data. In neural networks, overfitting is the phenomenon where the model learns patterns that are too similar to the actual patterns. This is known as a “one-hot encoding” problem, where the model is unable to differentiate between the different patterns. \n\nWhen overfitting occurs, the model learns patterns that are too similar to the actual patterns. The model is unable to differentiate between the different patterns, and it becomes less accurate. This is because the model is trying to predict patterns that are similar to the patterns it has already learned.\n\nTo prevent overfitting, it is important to properly train the model. This involves training the model with a larger amount of data, using a larger number of epochs, and using different learning algorithms.\n\nIn neural networks, overfitting is a problem in the early stages of the training process.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"trainer_base = Trainer(\n    model=base_model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T11:17:09.920831Z","iopub.execute_input":"2026-02-11T11:17:09.921433Z","iopub.status.idle":"2026-02-11T11:17:09.928681Z","shell.execute_reply.started":"2026-02-11T11:17:09.921401Z","shell.execute_reply":"2026-02-11T11:17:09.927928Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Comparison between Base and finetune V1","metadata":{}},{"cell_type":"code","source":"print(\"FOR BASE MODEL\")\neval_results = trainer_base.evaluate()\nperplexity = math.exp(eval_results[\"eval_loss\"])\n\nprint(f\"Eval Loss: {eval_results['eval_loss']}\")\nprint(f\"Perplexity: {perplexity}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T11:17:10.297203Z","iopub.execute_input":"2026-02-11T11:17:10.297443Z","iopub.status.idle":"2026-02-11T11:17:22.820752Z","shell.execute_reply.started":"2026-02-11T11:17:10.297420Z","shell.execute_reply":"2026-02-11T11:17:22.820129Z"}},"outputs":[{"name":"stdout","text":"FOR BASE MODEL\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:12]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Eval Loss: 2.124366044998169\nPerplexity: 8.367591130058685\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## experiment and hyper parameter tuning for finding the best lora config","metadata":{}},{"cell_type":"code","source":"import torch\nimport math\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\n\ndef run_experiment(r_value=8, dataset_size=1500, epochs=3):\n    \n    print(f\"\\nRunning Experiment | r={r_value} | dataset={dataset_size} | epochs={epochs}\")\n    \n    # 1️⃣ Load fresh base model\n    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        torch_dtype=torch.float32,\n        device_map=\"auto\"\n    )\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.padding_side = \"right\"\n    \n    # 2️⃣ Inject LoRA\n    lora_config = LoraConfig(\n        r=r_value,\n        lora_alpha=r_value * 2,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    \n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()\n    \n    # 3️⃣ Load dataset\n    dataset = load_dataset(\"databricks/databricks-dolly-15k\")\n    dataset = dataset[\"train\"].shuffle(seed=42).select(range(dataset_size))\n    \n    split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n    train_dataset = split_dataset[\"train\"]\n    eval_dataset = split_dataset[\"test\"]\n    \n    # 4️⃣ Format prompts\n    def format_prompt(example):\n        if example[\"context\"]:\n            prompt = f\"\"\"### Instruction:\n{example['instruction']}\n\n### Context:\n{example['context']}\n\n### Response:\n{example['response']}\"\"\"\n        else:\n            prompt = f\"\"\"### Instruction:\n{example['instruction']}\n\n### Response:\n{example['response']}\"\"\"\n        return {\"text\": prompt}\n    \n    train_dataset = train_dataset.map(format_prompt)\n    eval_dataset = eval_dataset.map(format_prompt)\n    \n    # 5️⃣ Tokenize\n    max_length = 512\n    \n    def tokenize_function(example):\n        return tokenizer(\n            example[\"text\"],\n            truncation=True,\n            max_length=max_length,\n            padding=\"max_length\"\n        )\n    \n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n    \n    train_dataset = train_dataset.remove_columns(\n        [\"instruction\", \"context\", \"response\", \"category\", \"text\"]\n    )\n    eval_dataset = eval_dataset.remove_columns(\n        [\"instruction\", \"context\", \"response\", \"category\", \"text\"]\n    )\n    \n    train_dataset.set_format(\"torch\")\n    eval_dataset.set_format(\"torch\")\n    \n    # 6️⃣ Training setup\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n    \n    training_args = TrainingArguments(\n        output_dir=f\"./exp_r{r_value}_{dataset_size}\",\n        per_device_train_batch_size=2,\n        per_device_eval_batch_size=2,\n        gradient_accumulation_steps=8,\n        num_train_epochs=epochs,\n        learning_rate=2e-4,\n        logging_steps=100,\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",\n        fp16=True,\n        report_to=\"none\"\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        data_collator=data_collator\n    )\n    \n    trainer.train()\n    \n    # 7️⃣ Evaluate\n    eval_results = trainer.evaluate()\n    perplexity = math.exp(eval_results[\"eval_loss\"])\n    \n    print(f\"Eval Loss: {eval_results['eval_loss']}\")\n    print(f\"Perplexity: {perplexity}\")\n    \n    return perplexity\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T19:34:43.955353Z","iopub.execute_input":"2026-02-11T19:34:43.956410Z","iopub.status.idle":"2026-02-11T19:34:43.968395Z","shell.execute_reply.started":"2026-02-11T19:34:43.956369Z","shell.execute_reply":"2026-02-11T19:34:43.967707Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"ppl_3000_r16 = run_experiment(r_value=16, dataset_size=3000, epochs=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T17:06:38.463598Z","iopub.execute_input":"2026-02-11T17:06:38.464256Z","iopub.status.idle":"2026-02-11T18:04:51.013529Z","shell.execute_reply.started":"2026-02-11T17:06:38.464222Z","shell.execute_reply":"2026-02-11T18:04:51.012736Z"}},"outputs":[{"name":"stdout","text":"\nRunning Experiment | r=16 | dataset=3000 | epochs=3\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"526938c1a1ee4c47bce3592d8f5121ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87ee41f3b29c4c8eb219b56837a25204"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea74c70168394e0a8e563461203f04bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7b097fcf0a94682806b95334bad4a65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fa7af5403d344d8b3bf7c0b47ce4de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5003eb044c59499c9c8b09203cc0c3d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2cd7fbf01343d8855f87622c251947"}},"metadata":{}},{"name":"stdout","text":"trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.20437245579516677\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4447ee0a09b44f2948d706a7bf8143f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3502d75d1e7f4b409109df3b69f6bbea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52334b65d25a413fac0ecf7cdd2c4924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bd28e210e7f4b1f91d534a60b33a08b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ed4fefd98b949fab351ac7ce4be44ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2700 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ca1c5d2f56492099f56da8896cdd39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5dbcc3e5c6b4324a592391f4a958b11"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='504' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [504/504 56:46, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.736900</td>\n      <td>1.702655</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.624500</td>\n      <td>1.690754</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.612200</td>\n      <td>1.688168</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 00:53]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Eval Loss: 1.6881675720214844\nPerplexity: 5.409558990248326\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"ppl_1500_r16 = run_experiment(r_value=16, dataset_size=1500, epochs=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T19:34:47.340287Z","iopub.execute_input":"2026-02-11T19:34:47.340582Z","iopub.status.idle":"2026-02-11T20:03:46.542385Z","shell.execute_reply.started":"2026-02-11T19:34:47.340556Z","shell.execute_reply":"2026-02-11T20:03:46.541640Z"}},"outputs":[{"name":"stdout","text":"\nRunning Experiment | r=16 | dataset=1500 | epochs=3\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.20437245579516677\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"625edcda33ec42e3b77d1a3a0c422585"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:469: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='252' max='252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [252/252 28:20, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>No log</td>\n      <td>1.704668</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.741100</td>\n      <td>1.696427</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.659700</td>\n      <td>1.695156</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:26]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Eval Loss: 1.6951563358306885\nPerplexity: 5.447497537698147\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"ppl_1500_r16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T20:22:34.258762Z","iopub.execute_input":"2026-02-11T20:22:34.259397Z","iopub.status.idle":"2026-02-11T20:22:34.264150Z","shell.execute_reply.started":"2026-02-11T20:22:34.259369Z","shell.execute_reply":"2026-02-11T20:22:34.263449Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"5.447497537698147"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"model.save_pretrained(\"tinyllama-lora-adapter\")\ntokenizer.save_pretrained(\"tinyllama-lora-adapter\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-11T20:18:23.374228Z","iopub.execute_input":"2026-02-11T20:18:23.375033Z","iopub.status.idle":"2026-02-11T20:18:23.381833Z","shell.execute_reply.started":"2026-02-11T20:18:23.374996Z","shell.execute_reply":"2026-02-11T20:18:23.380862Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2362226599.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tinyllama-lora-adapter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tinyllama-lora-adapter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"!pip freeze > requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}